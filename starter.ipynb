{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numba as nb\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space:  Box(0, 255, (96, 96, 3), uint8)\n",
      "Action space:  Discrete(5)\n"
     ]
    }
   ],
   "source": [
    "# Set up environment\n",
    "env = gym.make('CarRacing-v2', continuous=False)\n",
    "# Then we reset this environment\n",
    "# observation = env.reset()\n",
    "\n",
    "\n",
    "print(\"Observation space: \", env.observation_space)\n",
    "print(\"Action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 96, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s, info = env.reset()\n",
    "print(s.shape)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(s)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img):\n",
    "    img = img[:84, 6:90] # CarRacing-v2-specific cropping\n",
    "    # img = cv2.resize(img, dsize=(84, 84)) # or you can simply use rescaling\n",
    "    \n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(fastmath=True)\n",
    "def rgb_to_grey(img):\n",
    "    \"\"\"\n",
    "    Convert an RGB image to greyscale using the weighted method.\n",
    "    \"\"\"\n",
    "    num_rows, num_cols, _ = img.shape\n",
    "    grey_img = np.empty((num_rows, num_cols), dtype=np.uint8)\n",
    "    for i, row in enumerate(img):\n",
    "        for j, rgb_pixel in enumerate(row):\n",
    "            # Compute weighted sum of RGB channels\n",
    "            grey_img[i, j] = 0.2989 * rgb_pixel[0] + 0.5870 * rgb_pixel[1] + 0.1140 * rgb_pixel[2]\n",
    "\n",
    "    return grey_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEnv(gym.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        skip_frames=4,\n",
    "        stack_frames=4,\n",
    "        initial_no_op=50,\n",
    "        # colours = np.array([[170,0,0],[105,230,105],[0,0,0],[101,101,101],[255,255,255]]),\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(ImageEnv, self).__init__(env, **kwargs)\n",
    "        self.initial_no_op = initial_no_op\n",
    "        self.skip_frames = skip_frames\n",
    "        self.stack_frames = stack_frames\n",
    "        # self.colours = colours\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        # reset the original environment\n",
    "        s, info = self.env.reset()\n",
    "\n",
    "        # Do nothing for the next `self.initial_no_op` steps\n",
    "        for i in range(self.initial_no_op):\n",
    "            s, r, terminated, truncated, info = self.env.step(0)\n",
    "\n",
    "        # crop image\n",
    "        s = crop(s)\n",
    "        s = rgb_to_grey(s)\n",
    "        \n",
    "\n",
    "        # initial observation is simply a copy of the frame 's'\n",
    "        self.stacked_state = np.tile(s, (self.stack_frames, 1, 1))\n",
    "        return self.stacked_state, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Take an action for self.skip_frames steps\n",
    "        reward = 0\n",
    "        for _ in range(self.skip_frames):\n",
    "            s, r, terminated, truncated, info = self.env.step(action)\n",
    "            reward += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        s = crop(s)\n",
    "        s = rgb_to_grey(s)\n",
    "\n",
    "        # push the current frame 's' at the end of self.stacked_state\n",
    "        self.stacked_state = np.concatenate((self.stacked_state[1:], s[np.newaxis]), axis=0)\n",
    "\n",
    "        return self.stacked_state, reward, terminated, truncated, info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of an observation:  (4, 84, 84)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CarRacing-v2', continuous=False)\n",
    "env = ImageEnv(env)\n",
    "\n",
    "s, _ = env.reset()\n",
    "print(\"The shape of an observation: \", s.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for i in range(4):\n",
    "    axes[i].imshow(s[i], cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNActionValue(tf.keras.Model):\n",
    "    def __init__(self, state_dim, action_dim, activation=tf.nn.relu):\n",
    "        super(CNNActionValue, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(16, kernel_size=8, strides=4, input_shape=state_dim) # [N, 4, 84, 84] -> [N, 16, 20, 20]\n",
    "        self.conv2 = tf.keras.layers.Conv2D(32, kernel_size=4, strides=2) # [N, 16, 20, 20] -> [N, 32, 9, 9]\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(256, activation=activation)\n",
    "        self.fc2 = tf.keras.layers.Dense(action_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.nn.relu(self.conv1(x))\n",
    "        x = tf.nn.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From paper: \"The input to the neural network consists is an 84 × 84 × 4 image produced by \n",
    ". The first hidden layer convolves 16 8 × 8 filters with stride 4 with the input image and applies a rectifier nonlinearity [10, 18]. The second hidden layer convolves 32 4 × 4 filters with stride 2, again followed by a rectifier nonlinearity. The final hidden layer is fully-connected and consists of 256 rectifier units. The output layer is a fullyconnected linear layer with a single output for each valid action\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay buffer used to store transitions experienced by the agent during its interaction with the environment.\n",
    "\n",
    "In a DQN, the agent learns by updating its Q-network weights using a batch of experiences collected from its replay buffer, rather than using each experience only once and discarding it. The replay buffer stores a large number of these experiences in a first-in, first-out (FIFO) queue, allowing the agent to learn from them multiple times.\n",
    "\n",
    "Each experience is a tuple of four elements: the current state, the action taken in that state, the reward obtained from that action, and the resulting next state. These experiences are collected during the agent's interaction with the environment and stored in the replay buffer.\n",
    "\n",
    "During training, the agent samples a mini-batch of experiences from the replay buffer at random and uses them to update its Q-network. This process of randomly sampling and reusing past experiences helps to break the correlation between consecutive updates and stabilizes the learning process.\n",
    "\n",
    "Replay buffers have been shown to be effective in improving the stability and convergence of DQN training, and are commonly used in deep reinforcement learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e5)):\n",
    "        self.s = np.zeros((max_size, *state_dim), dtype=np.float32)\n",
    "        self.a = np.zeros((max_size, *action_dim), dtype=np.int64)\n",
    "        self.r = np.zeros((max_size, 1), dtype=np.float32)\n",
    "        self.s_prime = np.zeros((max_size, *state_dim), dtype=np.float32)\n",
    "        self.terminated = np.zeros((max_size, 1), dtype=np.float32)\n",
    "\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def update(self, s, a, r, s_prime, terminated):\n",
    "        self.s[self.ptr] = s\n",
    "        self.a[self.ptr] = a\n",
    "        self.r[self.ptr] = r\n",
    "        self.s_prime[self.ptr] = s_prime\n",
    "        self.terminated[self.ptr] = terminated\n",
    "        \n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, batch_size)\n",
    "        return (\n",
    "            tf.constant(self.s[ind], dtype=tf.float32),\n",
    "            tf.constant(self.a[ind], dtype=tf.int32),\n",
    "            tf.constant(self.r[ind], dtype=tf.float32),\n",
    "            tf.constant(self.s_prime[ind], dtype=tf.float32),\n",
    "            tf.constant(self.terminated[ind], dtype=tf.float32),\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=0.00025,\n",
    "        epsilon=1.0,\n",
    "        epsilon_min=0.1,\n",
    "        gamma=0.99,\n",
    "        batch_size=32,\n",
    "        warmup_steps=5000,\n",
    "        buffer_size=int(1e5),\n",
    "        target_update_interval=10000,\n",
    "    ):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.target_update_interval = target_update_interval\n",
    "\n",
    "        self.network = CNNActionValue(state_dim[0], action_dim)\n",
    "        self.target_network = CNNActionValue(state_dim[0], action_dim)\n",
    "        self.target_network.set_weights(self.network.get_weights())\n",
    "        self.optimizer = tf.keras.optimizers.RMSprop(lr)\n",
    "\n",
    "        self.buffer = ReplayBuffer(state_dim, (1, ), buffer_size)\n",
    "\n",
    "        self.total_steps = 0\n",
    "        self.epsilon_decay = (epsilon - epsilon_min) / 1e6\n",
    "\n",
    "    @tf.function\n",
    "    def act(self, x, training=True):\n",
    "        if training and ((tf.random.uniform([]) < self.epsilon) or (self.total_steps < self.warmup_steps)):\n",
    "            a = tf.random.uniform([], maxval=self.action_dim, dtype=tf.int64)\n",
    "        else:\n",
    "            x = tf.expand_dims(x, axis=0)\n",
    "            q = self.network(x)\n",
    "            a = tf.argmax(q, axis=-1)\n",
    "        return a.numpy()[0]\n",
    "\n",
    "    @tf.function\n",
    "    def learn(self):\n",
    "        s, a, r, s_prime, terminated = self.buffer.sample(self.batch_size)\n",
    "        s = tf.convert_to_tensor(s)\n",
    "        a = tf.convert_to_tensor(a)\n",
    "        r = tf.convert_to_tensor(r)\n",
    "        s_prime = tf.convert_to_tensor(s_prime)\n",
    "        terminated = tf.convert_to_tensor(terminated)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            next_q = self.target_network(s_prime)\n",
    "            td_target = r + (1. - terminated) * self.gamma * tf.reduce_max(next_q, axis=-1, keepdims=True)\n",
    "            q_values = tf.gather_nd(self.network(s), tf.stack((tf.range(a.shape[0]), a[:, 0]), axis=1))\n",
    "            loss = tf.reduce_mean(tf.square(q_values - tf.stop_gradient(td_target)))\n",
    "\n",
    "        gradients = tape.gradient(loss, self.network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.network.trainable_variables))\n",
    "\n",
    "        result = {\n",
    "            'total_steps': self.total_steps,\n",
    "            'value_loss': loss.numpy(),\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    def process(self, transition):\n",
    "        result = {}\n",
    "        self.total_steps += 1\n",
    "        self.buffer.update(*transition)\n",
    "\n",
    "        if self.total_steps > self.warmup_steps:\n",
    "            result = self.learn()\n",
    "\n",
    "        if self.total_steps % self.target_update_interval == 0:\n",
    "            self.target_network.set_weights(self.network.get_weights())\n",
    "        self.epsilon -= self.epsilon_decay\n",
    "        return result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train DQN agent until the total number of interactions with the environment reaches 2 millilons and evaluate our agent evry 10,000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m state_dim \u001b[39m=\u001b[39m (\u001b[39m84\u001b[39m, \u001b[39m84\u001b[39m, \u001b[39m4\u001b[39m)  \n\u001b[0;32m      7\u001b[0m action_dim \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn\n\u001b[1;32m----> 9\u001b[0m agent \u001b[39m=\u001b[39m DQN(state_dim, action_dim)\n",
      "Cell \u001b[1;32mIn[59], line 24\u001b[0m, in \u001b[0;36mDQN.__init__\u001b[1;34m(self, state_dim, action_dim, lr, epsilon, epsilon_min, gamma, batch_size, warmup_steps, buffer_size, target_update_interval)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarmup_steps \u001b[39m=\u001b[39m warmup_steps\n\u001b[0;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_update_interval \u001b[39m=\u001b[39m target_update_interval\n\u001b[1;32m---> 24\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork \u001b[39m=\u001b[39m CNNActionValue(state_dim[\u001b[39m0\u001b[39;49m], action_dim)\n\u001b[0;32m     25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_network \u001b[39m=\u001b[39m CNNActionValue(state_dim[\u001b[39m0\u001b[39m], action_dim)\n\u001b[0;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_network\u001b[39m.\u001b[39mset_weights(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mget_weights())\n",
      "Cell \u001b[1;32mIn[43], line 4\u001b[0m, in \u001b[0;36mCNNActionValue.__init__\u001b[1;34m(self, state_dim, action_dim, activation)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, state_dim, action_dim, activation\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mrelu):\n\u001b[0;32m      3\u001b[0m     \u001b[39msuper\u001b[39m(CNNActionValue, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m----> 4\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1 \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mConv2D(\u001b[39m16\u001b[39;49m, kernel_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m, strides\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, input_shape\u001b[39m=\u001b[39;49mstate_dim) \u001b[39m# [N, 4, 84, 84] -> [N, 16, 20, 20]\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2 \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mConv2D(\u001b[39m32\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, strides\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m# [N, 16, 20, 20] -> [N, 32, 9, 9]\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mFlatten()\n",
      "File \u001b[1;32mc:\\Users\\sebsj\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\dtensor\\utils.py:96\u001b[0m, in \u001b[0;36mallow_initializer_layout.<locals>._wrap_function\u001b[1;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[39mif\u001b[39;00m layout:\n\u001b[0;32m     94\u001b[0m             layout_args[variable_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_layout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m layout\n\u001b[1;32m---> 96\u001b[0m init_method(layer_instance, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     98\u001b[0m \u001b[39m# Inject the layout parameter after the invocation of __init__()\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m layout_param_name, layout \u001b[39min\u001b[39;00m layout_args\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\sebsj\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\convolutional\\conv2d.py:179\u001b[0m, in \u001b[0;36mConv2D.__init__\u001b[1;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, groups, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[39m@utils\u001b[39m\u001b[39m.\u001b[39mallow_initializer_layout\n\u001b[0;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    160\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    178\u001b[0m ):\n\u001b[1;32m--> 179\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    180\u001b[0m         rank\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m    181\u001b[0m         filters\u001b[39m=\u001b[39mfilters,\n\u001b[0;32m    182\u001b[0m         kernel_size\u001b[39m=\u001b[39mkernel_size,\n\u001b[0;32m    183\u001b[0m         strides\u001b[39m=\u001b[39mstrides,\n\u001b[0;32m    184\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m    185\u001b[0m         data_format\u001b[39m=\u001b[39mdata_format,\n\u001b[0;32m    186\u001b[0m         dilation_rate\u001b[39m=\u001b[39mdilation_rate,\n\u001b[0;32m    187\u001b[0m         groups\u001b[39m=\u001b[39mgroups,\n\u001b[0;32m    188\u001b[0m         activation\u001b[39m=\u001b[39mactivations\u001b[39m.\u001b[39mget(activation),\n\u001b[0;32m    189\u001b[0m         use_bias\u001b[39m=\u001b[39muse_bias,\n\u001b[0;32m    190\u001b[0m         kernel_initializer\u001b[39m=\u001b[39minitializers\u001b[39m.\u001b[39mget(kernel_initializer),\n\u001b[0;32m    191\u001b[0m         bias_initializer\u001b[39m=\u001b[39minitializers\u001b[39m.\u001b[39mget(bias_initializer),\n\u001b[0;32m    192\u001b[0m         kernel_regularizer\u001b[39m=\u001b[39mregularizers\u001b[39m.\u001b[39mget(kernel_regularizer),\n\u001b[0;32m    193\u001b[0m         bias_regularizer\u001b[39m=\u001b[39mregularizers\u001b[39m.\u001b[39mget(bias_regularizer),\n\u001b[0;32m    194\u001b[0m         activity_regularizer\u001b[39m=\u001b[39mregularizers\u001b[39m.\u001b[39mget(activity_regularizer),\n\u001b[0;32m    195\u001b[0m         kernel_constraint\u001b[39m=\u001b[39mconstraints\u001b[39m.\u001b[39mget(kernel_constraint),\n\u001b[0;32m    196\u001b[0m         bias_constraint\u001b[39m=\u001b[39mconstraints\u001b[39m.\u001b[39mget(bias_constraint),\n\u001b[0;32m    197\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    198\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\sebsj\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py:118\u001b[0m, in \u001b[0;36mConv.__init__\u001b[1;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, groups, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, conv_op, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     95\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     96\u001b[0m     rank,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    117\u001b[0m ):\n\u001b[1;32m--> 118\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    119\u001b[0m         trainable\u001b[39m=\u001b[39mtrainable,\n\u001b[0;32m    120\u001b[0m         name\u001b[39m=\u001b[39mname,\n\u001b[0;32m    121\u001b[0m         activity_regularizer\u001b[39m=\u001b[39mregularizers\u001b[39m.\u001b[39mget(activity_regularizer),\n\u001b[0;32m    122\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    124\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrank \u001b[39m=\u001b[39m rank\n\u001b[0;32m    126\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(filters, \u001b[39mfloat\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\sebsj\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sebsj\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\base_layer.py:453\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[1;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    452\u001b[0m             batch_size \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 453\u001b[0m         batch_input_shape \u001b[39m=\u001b[39m (batch_size,) \u001b[39m+\u001b[39m \u001b[39mtuple\u001b[39;49m(kwargs[\u001b[39m\"\u001b[39;49m\u001b[39minput_shape\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    454\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_input_shape \u001b[39m=\u001b[39m batch_input_shape\n\u001b[0;32m    456\u001b[0m \u001b[39m# Manage initial weight values if passed.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "env = gym.make('CarRacing-v2', continuous=False)\n",
    "env = ImageEnv(env)\n",
    "\n",
    "max_steps = int(2e6)\n",
    "eval_interval = 10000\n",
    "state_dim = (84, 84, 4)  \n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = DQN(state_dim, action_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
