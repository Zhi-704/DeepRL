{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numba as nb\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]], dtype=uint8),\n",
       " {})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up environment\n",
    "env = gym.make('CarRacing-v2', continuous=False)\n",
    "env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img):\n",
    "    img = img[:84, 6:90] # CarRacing-v2-specific cropping\n",
    "    # img = cv2.resize(img, dsize=(84, 84)) # or you can simply use rescaling\n",
    "    \n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
    "    return img\n",
    "\n",
    "@nb.njit(fastmath=True)\n",
    "def rgb_to_grey(img):\n",
    "    \"\"\"\n",
    "    Convert an RGB image to greyscale using the weighted method.\n",
    "    \"\"\"\n",
    "    num_rows, num_cols, _ = img.shape\n",
    "    grey_img = np.empty((num_rows, num_cols), dtype=np.uint8)\n",
    "    for i, row in enumerate(img):\n",
    "        for j, rgb_pixel in enumerate(row):\n",
    "            # Compute weighted sum of RGB channels\n",
    "            grey_img[i, j] = 0.2989 * rgb_pixel[0] + 0.5870 * rgb_pixel[1] + 0.1140 * rgb_pixel[2]\n",
    "\n",
    "    return grey_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEnv(gym.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        skip_frames=4,\n",
    "        stack_frames=4,\n",
    "        initial_no_op=50,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(ImageEnv, self).__init__(env, **kwargs)\n",
    "        self.initial_no_op = initial_no_op\n",
    "        self.skip_frames = skip_frames\n",
    "        self.stack_frames = stack_frames\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        # reset the original environment\n",
    "        s, info = self.env.reset()\n",
    "\n",
    "        # Do nothing for the next `self.initial_no_op` steps\n",
    "        for i in range(self.initial_no_op):\n",
    "            s, r, terminated, truncated, info = self.env.step(0)\n",
    "\n",
    "        # crop image\n",
    "        s = crop(s)\n",
    "        s = rgb_to_grey(s)\n",
    "        \n",
    "\n",
    "        # initial observation is simply a copy of the frame 's'\n",
    "        self.stacked_state = np.tile(s, (self.stack_frames, 1, 1))\n",
    "        return self.stacked_state, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Take an action for self.skip_frames steps\n",
    "        reward = 0\n",
    "        for _ in range(self.skip_frames):\n",
    "            s, r, terminated, truncated, info = self.env.step(action)\n",
    "            reward += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        s = crop(s)\n",
    "        s = rgb_to_grey(s)\n",
    "\n",
    "        # push the current frame 's' at the end of self.stacked_state\n",
    "        self.stacked_state = np.concatenate((self.stacked_state[1:], s[np.newaxis]), axis=0)\n",
    "\n",
    "        return self.stacked_state, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of an observation:  (4, 84, 84)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CarRacing-v2', continuous=False)\n",
    "env = ImageEnv(env)\n",
    "\n",
    "s, _ = env.reset()\n",
    "print(\"The shape of an observation: \", s.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for i in range(4):\n",
    "    axes[i].imshow(s[i], cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNActionValue(tf.keras.Model):\n",
    "    def __init__(self, state_dim, action_dim, activation=tf.nn.relu):\n",
    "        super(CNNActionValue, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(16, kernel_size=8, strides=4, input_shape=state_dim) # [N, 4, 84, 84] -> [N, 16, 20, 20]\n",
    "        self.conv2 = tf.keras.layers.Conv2D(32, kernel_size=4, strides=2) # [N, 16, 20, 20] -> [N, 32, 9, 9]\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(256, activation=activation)\n",
    "        self.fc2 = tf.keras.layers.Dense(action_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.nn.relu(self.conv1(x))\n",
    "        x = tf.nn.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss=0.0391\n",
      "Iteration 2: Loss=-0.2181\n",
      "Iteration 3: Loss=-0.6831\n",
      "Iteration 4: Loss=-1.6845\n",
      "Iteration 5: Loss=-3.6693\n",
      "Iteration 6: Loss=-7.2227\n",
      "Iteration 7: Loss=-13.0740\n",
      "Iteration 8: Loss=-22.1340\n",
      "Iteration 9: Loss=-35.5198\n",
      "Iteration 10: Loss=-54.5563\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Generate some random input data\n",
    "input_data = np.random.rand(32, 84, 84, 4).astype(np.float32)\n",
    "\n",
    "input_data = tf.convert_to_tensor(input_data)\n",
    "\n",
    "# Step 2: Instantiate the model\n",
    "model = CNNActionValue(state_dim=(84, 84, 4), action_dim=10)\n",
    "\n",
    "# Step 3: Test the model's output\n",
    "output = model.call(input_data)\n",
    "assert output.shape == (32, 10), f\"Expected output shape (32, 10), but got {output.shape}\"\n",
    "\n",
    "# Step 4: Test the model's differentiability\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(input_data)\n",
    "    output = model.call(input_data)\n",
    "grads = tape.gradient(output, input_data)\n",
    "assert grads.shape == input_data.shape, f\"Expected gradient shape {input_data.shape}, but got {grads.shape}\"\n",
    "\n",
    "# Step 5: Update the model parameters\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "for i in range(10):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        output = model.call(input_data)\n",
    "        loss = tf.reduce_mean(output)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    # Step 6: Repeat the process for a few iterations\n",
    "    print(f\"Iteration {i+1}: Loss={loss.numpy():.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e5)):\n",
    "        self.s = np.zeros((max_size, *state_dim), dtype=np.float32)\n",
    "        self.a = np.zeros((max_size, *action_dim), dtype=np.int64)\n",
    "        self.r = np.zeros((max_size, 1), dtype=np.float32)\n",
    "        self.s_prime = np.zeros((max_size, *state_dim), dtype=np.float32)\n",
    "        self.terminated = np.zeros((max_size, 1), dtype=np.float32)\n",
    "\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def update(self, s, a, r, s_prime, terminated):\n",
    "        self.s[self.ptr] = s\n",
    "        self.a[self.ptr] = a\n",
    "        self.r[self.ptr] = r\n",
    "        self.s_prime[self.ptr] = s_prime\n",
    "        self.terminated[self.ptr] = terminated\n",
    "        \n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, batch_size)\n",
    "        return (\n",
    "            tf.constant(self.s[ind], dtype=tf.float32),\n",
    "            tf.constant(self.a[ind], dtype=tf.int32),\n",
    "            tf.constant(self.r[ind], dtype=tf.float32),\n",
    "            tf.constant(self.s_prime[ind], dtype=tf.float32),\n",
    "            tf.constant(self.terminated[ind], dtype=tf.float32),\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        learning_rate=0.00025,\n",
    "        epsilon=1.0,\n",
    "        epsilon_min=0.1,\n",
    "        gamma=0.99,\n",
    "        batch_size=32,\n",
    "        warmup_steps=5000,\n",
    "        buffer_size=int(1e5),\n",
    "        target_update_interval=10000,\n",
    "    ):\n",
    "        self.action_dim = action_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.target_update_interval = target_update_interval\n",
    "\n",
    "        self.network = CNNActionValue(state_dim, action_dim)\n",
    "        self.target_network = CNNActionValue(state_dim, action_dim)\n",
    "        self.target_network.set_weights(self.network.get_weights())\n",
    "        self.optimizer = tf.keras.optimizers.RMSprop(learning_rate)\n",
    "\n",
    "        self.buffer = ReplayBuffer(state_dim, (1, ), buffer_size)\n",
    "        self.device = tf.device('gpu' if tf.test.is_built_with_cuda() else 'cpu')\n",
    "\n",
    "        self.total_steps = 0\n",
    "        self.epsilon_decay = (epsilon - epsilon_min) / 1e6\n",
    "\n",
    "    # @tf.function\n",
    "    # def act(self, x, training=True):\n",
    "    #     self.network.trainable(training)\n",
    "    #     if training and ((tf.random.uniform([]) < self.epsilon) or (self.total_steps < self.warmup_steps)):\n",
    "    #         a = tf.random.uniform([], maxval=self.action_dim, dtype=tf.int64)\n",
    "    #         # np.random.randint(self.action_dim)\n",
    "    #     else:\n",
    "    #         x = tf.expand_dims(x, axis=0)\n",
    "    #         q = self.network(x)\n",
    "    #         a = tf.argmax(q, axis=-1)\n",
    "    #     return a.numpy()[0]\n",
    "    \n",
    "    \n",
    "    def act(self, x, training=True):\n",
    "        if not training:\n",
    "            tf.keras.backend.set_learning_phase(False)\n",
    "        else:\n",
    "            tf.keras.backend.set_learning_phase(True)\n",
    "\n",
    "        if training and ((np.random.rand() < self.epsilon) or (self.total_steps < self.warmup_steps)):\n",
    "            a = np.random.randint(0, self.action_dim)\n",
    "        else:\n",
    "            x = tf.expand_dims(tf.convert_to_tensor(x, dtype=tf.float32), 0)\n",
    "            q = self.network(x, training=training)\n",
    "            a = tf.argmax(q[0]).numpy()\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def learn(self):\n",
    "        s, a, r, s_prime, terminated = self.buffer.sample(self.batch_size)\n",
    "        s = tf.convert_to_tensor(s)\n",
    "        a = tf.convert_to_tensor(a)\n",
    "        r = tf.convert_to_tensor(r)\n",
    "        s_prime = tf.convert_to_tensor(s_prime)\n",
    "        terminated = tf.convert_to_tensor(terminated)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            next_q = self.target_network(s_prime)\n",
    "            td_target = r + (1. - terminated) * self.gamma * tf.reduce_max(next_q, axis=-1, keepdims=True)\n",
    "            q_values = tf.gather_nd(self.network(s), tf.stack((tf.range(a.shape[0]), a[:, 0]), axis=1))\n",
    "            loss = tf.reduce_mean(tf.square(q_values - tf.stop_gradient(td_target)))\n",
    "\n",
    "        gradients = tape.gradient(loss, self.network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.network.trainable_variables))\n",
    "\n",
    "        result = {\n",
    "            'total_steps': self.total_steps,\n",
    "            'value_loss': loss.numpy(),\n",
    "        }\n",
    "        return result\n",
    "    \n",
    "    def process(self, transition):\n",
    "        result = {}\n",
    "        self.total_steps += 1\n",
    "        self.buffer.update(*transition)\n",
    "\n",
    "        if self.total_steps > self.warmup_steps:\n",
    "            result = self.learn()\n",
    "\n",
    "        if self.total_steps % self.target_update_interval == 0:\n",
    "            self.target_network.set_weights(self.network.get_weights())\n",
    "        self.epsilon -= self.epsilon_decay\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2', continuous=False)\n",
    "env = ImageEnv(env)\n",
    "\n",
    "max_steps = int(250000)\n",
    "eval_interval = 10000\n",
    "state_dim = (4, 84, 84)  \n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = DQN(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(n_evals=5):\n",
    "    eval_env = gym.make('CarRacing-v2', continuous=False)\n",
    "    eval_env = ImageEnv(eval_env)\n",
    "    \n",
    "    scores = 0\n",
    "    for i in range(n_evals):\n",
    "        (s, _), done, ret = eval_env.reset(), False, 0\n",
    "        while not done:\n",
    "            a = agent.act(s, training=False)\n",
    "            s_prime, r, terminated, truncated, info = eval_env.step(a)\n",
    "            s = s_prime\n",
    "            ret += r\n",
    "            done = terminated or truncated\n",
    "        scores += ret\n",
    "    return np.round(scores / n_evals, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebsj\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\sebsj\\AppData\\Local\\Temp\\ipykernel_21584\\1115987189.py\", line 71, in learn  *\n        next_q = self.target_network(s_prime)\n    File \"c:\\Users\\sebsj\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\sebsj\\AppData\\Local\\Temp\\__autograph_generated_file9dah2hm8.py\", line 10, in tf__call\n        x = ag__.converted_call(ag__.ld(tf).nn.relu, (ag__.converted_call(ag__.ld(self).conv1, (ag__.ld(x),), None, fscope),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'cnn_action_value_2' (type CNNActionValue).\n    \n    in user code:\n    \n        File \"C:\\Users\\sebsj\\AppData\\Local\\Temp\\ipykernel_21584\\1781783688.py\", line 11, in call  *\n            x = tf.nn.relu(self.conv1(x))\n        File \"c:\\Users\\sebsj\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer 'conv2d_4' (type Conv2D).\n        \n        Negative dimension size caused by subtracting 8 from 4 for '{{node cnn_action_value_2/conv2d_4/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 4, 4, 1], use_cudnn_on_gpu=true](Const_3, cnn_action_value_2/conv2d_4/Conv2D/ReadVariableOp)' with input shapes: [32,4,84,84], [8,8,84,16].\n        \n        Call arguments received by layer 'conv2d_4' (type Conv2D):\n          • inputs=tf.Tensor(shape=(32, 4, 84, 84), dtype=float32)\n    \n    \n    Call arguments received by layer 'cnn_action_value_2' (type CNNActionValue):\n      • x=tf.Tensor(shape=(32, 4, 84, 84), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m a \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mact(s)\n\u001b[0;32m      8\u001b[0m s_prime, r, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(a)\n\u001b[1;32m----> 9\u001b[0m result \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mprocess((s, a, r, s_prime, terminated))  \u001b[39m# You can track q-losses over training from `result` variable.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m s \u001b[39m=\u001b[39m s_prime\n\u001b[0;32m     12\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "Cell \u001b[1;32mIn[9], line 91\u001b[0m, in \u001b[0;36mDQN.process\u001b[1;34m(self, transition)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mupdate(\u001b[39m*\u001b[39mtransition)\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_steps \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarmup_steps:\n\u001b[1;32m---> 91\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn()\n\u001b[0;32m     93\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_steps \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_update_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     94\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_network\u001b[39m.\u001b[39mset_weights(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mget_weights())\n",
      "File \u001b[1;32mc:\\Users\\sebsj\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filesdt7ta36.py:17\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m terminated \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mconvert_to_tensor, (ag__\u001b[39m.\u001b[39mld(terminated),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mwith\u001b[39;00m ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m---> 17\u001b[0m     next_q \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mtarget_network, (ag__\u001b[39m.\u001b[39mld(s_prime),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     18\u001b[0m     td_target \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(r) \u001b[39m+\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m ag__\u001b[39m.\u001b[39mld(terminated)) \u001b[39m*\u001b[39m ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mreduce_max, (ag__\u001b[39m.\u001b[39mld(next_q),), \u001b[39mdict\u001b[39m(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), fscope)\n\u001b[0;32m     19\u001b[0m     q_values \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mgather_nd, (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mnetwork, (ag__\u001b[39m.\u001b[39mld(s),), \u001b[39mNone\u001b[39;00m, fscope), ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mstack, ((ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mrange, (ag__\u001b[39m.\u001b[39mld(a)\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],), \u001b[39mNone\u001b[39;00m, fscope), ag__\u001b[39m.\u001b[39mld(a)[:, \u001b[39m0\u001b[39m]),), \u001b[39mdict\u001b[39m(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), fscope)), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32mc:\\Users\\sebsj\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9dah2hm8.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mrelu, (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mconv1, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     11\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mrelu, (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mconv2, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mflatten, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\sebsj\\AppData\\Local\\Temp\\ipykernel_21584\\1115987189.py\", line 71, in learn  *\n        next_q = self.target_network(s_prime)\n    File \"c:\\Users\\sebsj\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\sebsj\\AppData\\Local\\Temp\\__autograph_generated_file9dah2hm8.py\", line 10, in tf__call\n        x = ag__.converted_call(ag__.ld(tf).nn.relu, (ag__.converted_call(ag__.ld(self).conv1, (ag__.ld(x),), None, fscope),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'cnn_action_value_2' (type CNNActionValue).\n    \n    in user code:\n    \n        File \"C:\\Users\\sebsj\\AppData\\Local\\Temp\\ipykernel_21584\\1781783688.py\", line 11, in call  *\n            x = tf.nn.relu(self.conv1(x))\n        File \"c:\\Users\\sebsj\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer 'conv2d_4' (type Conv2D).\n        \n        Negative dimension size caused by subtracting 8 from 4 for '{{node cnn_action_value_2/conv2d_4/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 4, 4, 1], use_cudnn_on_gpu=true](Const_3, cnn_action_value_2/conv2d_4/Conv2D/ReadVariableOp)' with input shapes: [32,4,84,84], [8,8,84,16].\n        \n        Call arguments received by layer 'conv2d_4' (type Conv2D):\n          • inputs=tf.Tensor(shape=(32, 4, 84, 84), dtype=float32)\n    \n    \n    Call arguments received by layer 'cnn_action_value_2' (type CNNActionValue):\n      • x=tf.Tensor(shape=(32, 4, 84, 84), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "history = {'Step': [], 'AvgReturn': []}\n",
    "\n",
    "(s, _) = env.reset()\n",
    "while True:\n",
    "    a = agent.act(s)\n",
    "    s_prime, r, terminated, truncated, info = env.step(a)\n",
    "    result = agent.process((s, a, r, s_prime, terminated))  # You can track q-losses over training from `result` variable.\n",
    "    \n",
    "    s = s_prime\n",
    "    if terminated or truncated:\n",
    "        s, _ = env.reset()\n",
    "        \n",
    "    if agent.total_steps % eval_interval == 0:\n",
    "        ret = evaluate()\n",
    "        history['Step'].append(agent.total_steps)\n",
    "        history['AvgReturn'].append(ret)\n",
    "        \n",
    "        clear_output()\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(history['Step'], history['AvgReturn'], 'r-')\n",
    "        plt.xlabel('Step', fontsize=16)\n",
    "        plt.ylabel('AvgReturn', fontsize=16)\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        plt.grid(axis='y')\n",
    "        plt.show()\n",
    "        \n",
    "        agent.network.save_weights('dqn.h5')\n",
    "    \n",
    "    if agent.total_steps > max_steps:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
