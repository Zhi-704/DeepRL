{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow implementation of code by 재야의 숨은 초보\n",
    "# This code was adapted from this page:\n",
    "# https://hiddenbeginner.github.io/study-notes/contents/tutorials/2023-04-20_CartRacing-v2_DQN.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numba as nb\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment\n",
    "env = gym.make('CarRacing-v2', continuous=False)\n",
    "env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implements this paper: https://arxiv.org/pdf/1312.5602.pdf "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img):\n",
    "    img = img[:84, 6:90] # CarRacing-v2-specific cropping\n",
    "    # img = cv2.resize(img, dsize=(84, 84)) # or you can simply use rescaling\n",
    "    \n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
    "    return img\n",
    "\n",
    "@nb.njit(fastmath=True)\n",
    "def rgb_to_grey(img):\n",
    "    \"\"\"\n",
    "    Convert an RGB image to greyscale using the weighted method.\n",
    "    \"\"\"\n",
    "    num_rows, num_cols, _ = img.shape\n",
    "    grey_img = np.empty((num_rows, num_cols), dtype=np.uint8)\n",
    "    for i, row in enumerate(img):\n",
    "        for j, rgb_pixel in enumerate(row):\n",
    "            # Compute weighted sum of RGB channels\n",
    "            grey_img[i, j] = 0.2989 * rgb_pixel[0] + 0.5870 * rgb_pixel[1] + 0.1140 * rgb_pixel[2]\n",
    "\n",
    "    return grey_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEnv(gym.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        skip_frames=4,\n",
    "        stack_frames=4,\n",
    "        initial_no_op=50,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(ImageEnv, self).__init__(env, **kwargs)\n",
    "        self.initial_no_op = initial_no_op\n",
    "        self.skip_frames = skip_frames\n",
    "        self.stack_frames = stack_frames\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        # reset the original environment\n",
    "        s, info = self.env.reset()\n",
    "\n",
    "        # Do nothing for the next `self.initial_no_op` steps\n",
    "        for i in range(self.initial_no_op):\n",
    "            s, r, terminated, truncated, info = self.env.step(0)\n",
    "\n",
    "        # crop image\n",
    "        s = crop(s)\n",
    "        s = rgb_to_grey(s)\n",
    "        \n",
    "\n",
    "        # initial observation is simply a copy of the frame 's'\n",
    "        self.stacked_state = np.tile(s, (self.stack_frames, 1, 1))\n",
    "        return self.stacked_state, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Take an action for self.skip_frames steps\n",
    "        reward = 0\n",
    "        for _ in range(self.skip_frames):\n",
    "            s, r, terminated, truncated, info = self.env.step(action)\n",
    "            reward += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        s = crop(s)\n",
    "        s = rgb_to_grey(s)\n",
    "\n",
    "        # push the current frame 's' at the end of self.stacked_state\n",
    "        self.stacked_state = np.concatenate((self.stacked_state[1:], s[np.newaxis]), axis=0)\n",
    "\n",
    "        return self.stacked_state, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2', continuous=False)\n",
    "env = ImageEnv(env)\n",
    "\n",
    "s, _ = env.reset()\n",
    "# print(env.observation_space.shape[0])\n",
    "print(\"The shape of an observation: \", s.shape)\n",
    "\n",
    "# fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "# for i in range(4):\n",
    "#     axes[i].imshow(s[i], cmap='gray')\n",
    "#     axes[i].axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNActionValue(tf.keras.Model):\n",
    "    def __init__(self, state_dim, action_dim, activation=tf.nn.relu):\n",
    "        super(CNNActionValue, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(16, kernel_size=8, strides=4, input_shape=state_dim) # [N, 4, 84, 84] -> [N, 16, 20, 20]\n",
    "        self.conv2 = tf.keras.layers.Conv2D(32, kernel_size=4, strides=2) # [N, 16, 20, 20] -> [N, 32, 9, 9]\n",
    "        # self.conv1 = tf.keras.layers.Conv2D(16, (8, 8), strides=4, activation='relu', input_shape=state_dim)\n",
    "        # self.conv2 = tf.keras.layers.Conv2D(32, (4, 4), strides=2, activation='relu')\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(256, activation=activation)\n",
    "        self.fc2 = tf.keras.layers.Dense(action_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        x = tf.nn.relu(self.conv1(x))\n",
    "        x = tf.nn.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate some random input data\n",
    "input_data = np.random.rand(32, 84, 84, 4).astype(np.float32)\n",
    "\n",
    "input_data = tf.convert_to_tensor(input_data)\n",
    "\n",
    "# Instantiate the model\n",
    "model = CNNActionValue(state_dim=(4, 84, 84), action_dim=10)\n",
    "\n",
    "# Test the model's output\n",
    "output = model.call(input_data)\n",
    "assert output.shape == (32, 10), f\"Expected output shape (32, 10), but got {output.shape}\"\n",
    "\n",
    "# Test the model's differentiability\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(input_data)\n",
    "    output = model.call(input_data)\n",
    "grads = tape.gradient(output, input_data)\n",
    "assert grads.shape == input_data.shape, f\"Expected gradient shape {input_data.shape}, but got {grads.shape}\"\n",
    "\n",
    "# Update the model parameters\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "for i in range(10):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        output = model.call(input_data)\n",
    "        loss = tf.reduce_mean(output)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "\n",
    "    print(f\"Iteration {i+1}: Loss={loss.numpy():.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "        self.s = np.zeros((max_size, *state_dim), dtype=np.float32)\n",
    "        self.a = np.zeros((max_size, *action_dim), dtype=np.int64)\n",
    "        self.r = np.zeros((max_size, 1), dtype=np.float32)\n",
    "        self.s_prime = np.zeros((max_size, *state_dim), dtype=np.float32)\n",
    "        self.terminated = np.zeros((max_size, 1), dtype=np.float32)\n",
    "\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def update(self, s, a, r, s_prime, terminated):\n",
    "        self.s[self.ptr] = s\n",
    "        self.a[self.ptr] = a\n",
    "        self.r[self.ptr] = r\n",
    "        self.s_prime[self.ptr] = s_prime\n",
    "        self.terminated[self.ptr] = terminated\n",
    "        \n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, batch_size)\n",
    "        return (\n",
    "            tf.constant(self.s[ind], dtype=tf.float32),\n",
    "            tf.constant(self.a[ind], dtype=tf.int32),\n",
    "            tf.constant(self.r[ind], dtype=tf.float32),\n",
    "            tf.constant(self.s_prime[ind], dtype=tf.float32),\n",
    "            tf.constant(self.terminated[ind], dtype=tf.float32),\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        learning_rate=0.00025,\n",
    "        epsilon=1.0,\n",
    "        epsilon_min=0.1,\n",
    "        gamma=0.99,\n",
    "        batch_size=32,\n",
    "        warmup_steps=5000,\n",
    "        buffer_size=int(1e5),\n",
    "        target_update_interval=10000,\n",
    "    ):\n",
    "        self.action_dim = action_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.target_update_interval = target_update_interval\n",
    "\n",
    "        self.network = CNNActionValue(state_dim, action_dim)\n",
    "        self.target_network = CNNActionValue(state_dim, action_dim)\n",
    "        self.target_network.set_weights(self.network.get_weights())\n",
    "        self.optimizer = tf.keras.optimizers.RMSprop(learning_rate)\n",
    "\n",
    "        self.buffer = ReplayBuffer(state_dim, (1, ), buffer_size)\n",
    "        self.device = tf.device('gpu' if tf.test.is_built_with_cuda() else 'cpu')\n",
    "\n",
    "        self.total_steps = 0\n",
    "        self.epsilon_decay = (epsilon - epsilon_min) / 1e6\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def act(self, x, training=True):\n",
    "        self.network.trainable = training\n",
    "        if training and ((np.random.rand() < self.epsilon) or (self.total_steps < self.warmup_steps)):\n",
    "            a = np.random.randint(0, self.action_dim)\n",
    "        else:\n",
    "            x = tf.expand_dims(tf.convert_to_tensor(x, dtype=tf.float32), axis=0)\n",
    "            q = self.network(x)\n",
    "            a = tf.argmax(q, axis=1).numpy()[0]\n",
    "        return a\n",
    "\n",
    "    # def act(self, observation):\n",
    "    #     if np.random.random() < self.epsilon:\n",
    "    #         action = np.random.choice(self.action_dim)\n",
    "    #     else:\n",
    "    #         state = np.array([observation])\n",
    "    #         actions = self.network(state)\n",
    "\n",
    "    #         action = np.argmax(actions)\n",
    "\n",
    "    #     return action\n",
    "        \n",
    "    \n",
    "    @tf.function\n",
    "    def learn(self):\n",
    "        s, a, r, s_prime, terminated = self.buffer.sample(self.batch_size)\n",
    "        s = tf.convert_to_tensor(s)\n",
    "        a = tf.convert_to_tensor(a)\n",
    "        r = tf.convert_to_tensor(r)\n",
    "        s_prime = tf.convert_to_tensor(s_prime)\n",
    "        terminated = tf.convert_to_tensor(terminated)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            next_q = self.target_network(s_prime)\n",
    "            td_target = r + (1. - terminated) * self.gamma * tf.reduce_max(next_q, axis=-1, keepdims=True)\n",
    "            q_values = tf.gather_nd(self.network(s), tf.stack((tf.range(a.shape[0]), a[:, 0]), axis=1))\n",
    "            loss = tf.reduce_mean(tf.square(q_values - tf.stop_gradient(td_target)))\n",
    "\n",
    "        gradients = tape.gradient(loss, self.network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.network.trainable_variables))\n",
    "\n",
    "        result = {\n",
    "            'total_steps': self.total_steps,\n",
    "            'value_loss': loss.numpy(),\n",
    "        }\n",
    "        return result\n",
    "    \n",
    "    def process(self, transition):\n",
    "        result = {}\n",
    "        self.total_steps += 1\n",
    "        self.buffer.update(*transition)\n",
    "\n",
    "        if self.total_steps > self.warmup_steps:\n",
    "            result = self.learn()\n",
    "\n",
    "        if self.total_steps % self.target_update_interval == 0:\n",
    "            self.target_network.set_weights(self.network.get_weights())\n",
    "        self.epsilon -= self.epsilon_decay\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2', continuous=False)\n",
    "env = ImageEnv(env)\n",
    "\n",
    "max_steps = int(2e6)\n",
    "eval_interval = 10000\n",
    "state_dim = (4, 84, 84) \n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = DQN(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(n_evals=5):\n",
    "    eval_env = gym.make('CarRacing-v2', continuous=False)\n",
    "    eval_env = ImageEnv(eval_env)\n",
    "    \n",
    "    scores = 0\n",
    "    for i in range(n_evals):\n",
    "        (s, _), done, ret = eval_env.reset(), False, 0\n",
    "        while not done:\n",
    "            a = agent.act(s)\n",
    "            s_prime, r, terminated, truncated, info = eval_env.step(a)\n",
    "            s = s_prime\n",
    "            ret += r\n",
    "            done = terminated or truncated\n",
    "        scores += ret\n",
    "    return np.round(scores / n_evals, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "history = {'Step': [], 'AvgReturn': []}\n",
    "\n",
    "(s, _) = env.reset()\n",
    "while True:\n",
    "    a = agent.act(s)\n",
    "    # print(a)\n",
    "    s_prime, r, terminated, truncated, info = env.step(int(a))\n",
    "    result = agent.process((s, a, r, s_prime, terminated))  # You can track q-losses over training from `result` variable.\n",
    "    \n",
    "    s = s_prime\n",
    "    if terminated or truncated:\n",
    "        s, _ = env.reset()\n",
    "        \n",
    "    if agent.total_steps % eval_interval == 0:\n",
    "        ret = evaluate()\n",
    "        history['Step'].append(agent.total_steps)\n",
    "        history['AvgReturn'].append(ret)\n",
    "        \n",
    "        clear_output()\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(history['Step'], history['AvgReturn'], 'r-')\n",
    "        plt.xlabel('Step', fontsize=16)\n",
    "        plt.ylabel('AvgReturn', fontsize=16)\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        plt.grid(axis='y')\n",
    "        plt.show()\n",
    "        \n",
    "        agent.network.save_weights('dqn.h5')\n",
    "    \n",
    "    if agent.total_steps > max_steps:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidAction",
     "evalue": "you passed the invalid action `2`. The supported action_space is `Discrete(5)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidAction\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     10\u001b[0m a \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mact(s)\n\u001b[1;32m---> 11\u001b[0m s_prime, r, terminated, truncated, info \u001b[39m=\u001b[39m eval_env\u001b[39m.\u001b[39;49mstep(a)\n\u001b[0;32m     12\u001b[0m s \u001b[39m=\u001b[39m s_prime\n\u001b[0;32m     13\u001b[0m ret \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m r\n",
      "Cell \u001b[1;32mIn[3], line 37\u001b[0m, in \u001b[0;36mImageEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_frames):\n\u001b[1;32m---> 37\u001b[0m     s, r, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     38\u001b[0m     reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m r\n\u001b[0;32m     39\u001b[0m     \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "File \u001b[1;32mc:\\Users\\sebsj\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\sebsj\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\sebsj\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\sebsj\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:541\u001b[0m, in \u001b[0;36mCarRacing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    540\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mcontains(action):\n\u001b[1;32m--> 541\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidAction(\n\u001b[0;32m    542\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39myou passed the invalid action `\u001b[39m\u001b[39m{\u001b[39;00maction\u001b[39m}\u001b[39;00m\u001b[39m`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    543\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe supported action_space is `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    544\u001b[0m         )\n\u001b[0;32m    545\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcar\u001b[39m.\u001b[39msteer(\u001b[39m-\u001b[39m\u001b[39m0.6\u001b[39m \u001b[39m*\u001b[39m (action \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m0.6\u001b[39m \u001b[39m*\u001b[39m (action \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m))\n\u001b[0;32m    546\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcar\u001b[39m.\u001b[39mgas(\u001b[39m0.2\u001b[39m \u001b[39m*\u001b[39m (action \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m))\n",
      "\u001b[1;31mInvalidAction\u001b[0m: you passed the invalid action `2`. The supported action_space is `Discrete(5)`"
     ]
    }
   ],
   "source": [
    "eval_env = gym.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n",
    "eval_env = ImageEnv(eval_env)\n",
    "\n",
    "frames = []\n",
    "scores = 0\n",
    "(s, _), done, ret = eval_env.reset(), False, 0\n",
    "while not done:\n",
    "    frames.append(eval_env.render())\n",
    "    s = s.astype(np.float32)\n",
    "    a = agent.act(s)\n",
    "    s_prime, r, terminated, truncated, info = eval_env.step(a)\n",
    "    s = s_prime\n",
    "    ret += r\n",
    "    done = terminated or truncated\n",
    "scores += ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(imgs, video_name, _return=True):\n",
    "    import cv2\n",
    "    import os\n",
    "    import string\n",
    "    import random\n",
    "    \n",
    "    if video_name is None:\n",
    "        video_name = ''.join(random.choice(string.ascii_letters) for i in range(18)) + '.webm'\n",
    "    height, width, layers = imgs[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'VP90')\n",
    "    video = cv2.VideoWriter(video_name, fourcc, 10, (width, height))\n",
    "    \n",
    "    for img in imgs:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        video.write(img)\n",
    "    video.release()\n",
    "    if _return:\n",
    "        from IPython.display import Video\n",
    "        return Video(video_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate(frames, \"my_video.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
