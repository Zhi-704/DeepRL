{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "909909a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium in ./.local/lib/python3.9/site-packages (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /opt/anaconda3/lib/python3.9/site-packages (from gymnasium) (4.11.3)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./.local/lib/python3.9/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/anaconda3/lib/python3.9/site-packages (from gymnasium) (1.21.5)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./.local/lib/python3.9/site-packages (from gymnasium) (4.5.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from gymnasium) (2.0.0)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in ./.local/lib/python3.9/site-packages (from gymnasium) (1.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium[box2d] in ./.local/lib/python3.9/site-packages (0.28.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from gymnasium[box2d]) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /opt/anaconda3/lib/python3.9/site-packages (from gymnasium[box2d]) (4.11.3)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./.local/lib/python3.9/site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/anaconda3/lib/python3.9/site-packages (from gymnasium[box2d]) (1.21.5)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in ./.local/lib/python3.9/site-packages (from gymnasium[box2d]) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./.local/lib/python3.9/site-packages (from gymnasium[box2d]) (4.5.0)\n",
      "Collecting box2d-py==2.3.5\n",
      "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
      "\u001b[K     |████████████████████████████████| 374 kB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting swig==4.*\n",
      "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 25.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pygame==2.1.3\n",
      "  Downloading pygame-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 33.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gymnasium[box2d]) (3.7.0)\n",
      "Building wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp39-cp39-linux_x86_64.whl size=494480 sha256=6ffdd2ed61519ffd43b69a836c22283f07bdc5957e7947db9fc17ab355c1a3bd\n",
      "  Stored in directory: /u/d/eh2169/.cache/pip/wheels/a4/c2/c1/076651c394f05fe60990cd85616c2d95bc1619aa113f559d7d\n",
      "Successfully built box2d-py\n",
      "Installing collected packages: swig, pygame, box2d-py\n",
      "\u001b[33m  WARNING: The script swig is installed in '/u/d/eh2169/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed box2d-py-2.3.5 pygame-2.1.3 swig-4.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium\n",
    "%pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a220e1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 15:36:49.213460: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from collections import deque\n",
    "import random\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b241aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img):\n",
    "    img = img[:84, 6:90]\n",
    "    return img\n",
    "\n",
    "@nb.njit(fastmath=True)\n",
    "def rgb_to_grey(img):\n",
    "    \"\"\"\n",
    "    Convert an RGB image to greyscale using the weighted method.\n",
    "    \"\"\"\n",
    "    num_rows, num_cols, _ = img.shape\n",
    "    grey_img = np.empty((num_rows, num_cols), dtype=np.uint32)\n",
    "    for i, row in enumerate(img):\n",
    "        for j, rgb_pixel in enumerate(row):\n",
    "            # Compute weighted sum of RGB channels\n",
    "            grey_img[i, j] = 0.2989 * rgb_pixel[0] + 0.5870 * rgb_pixel[1] + 0.1140 * rgb_pixel[2]\n",
    "\n",
    "    return grey_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1899c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    actor_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=8, strides=4, activation=\"relu\", kernel_initializer='zeros', input_shape=(84,84,1,)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Conv2D(128, kernel_size=4, strides=2, activation=\"relu\", kernel_initializer='zeros'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, activation=\"relu\", kernel_initializer='zeros'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(1024, kernel_initializer='zeros', activation=\"relu\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(3, kernel_initializer='zeros', activation=\"linear\")\n",
    "    ])\n",
    "    return actor_model\n",
    "\n",
    "def get_critic():\n",
    "    # Pass state through a convolutional neural network\n",
    "    state_in = tf.keras.layers.Input(shape=(84,84,1))\n",
    "    state_out = tf.keras.layers.Conv2D(64, kernel_size=8, strides=4, activation=\"relu\")(state_in)\n",
    "    state_out = tf.keras.layers.BatchNormalization()(state_out)\n",
    "    state_out = tf.keras.layers.Dropout(0.5)(state_out)\n",
    "    state_out = tf.keras.layers.Conv2D(128, kernel_size=4, strides=2, activation=\"relu\")(state_out)\n",
    "    state_out = tf.keras.layers.BatchNormalization()(state_out)\n",
    "    state_out = tf.keras.layers.Dropout(0.5)(state_out)\n",
    "    state_out = tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, activation=\"relu\")(state_out)\n",
    "    state_out = tf.keras.layers.BatchNormalization()(state_out)\n",
    "    state_out = tf.keras.layers.Dropout(0.5)(state_out)\n",
    "    state_out = tf.keras.layers.Flatten()(state_out)\n",
    "\n",
    "    # Pass action through a dense layer\n",
    "    action_in = tf.keras.layers.Input(shape=(3,))\n",
    "    action_out = tf.keras.layers.Dense(32, activation=\"relu\")(action_in)\n",
    "\n",
    "    # Concatenate state and action outputs and pass through dense layers\n",
    "    output = tf.keras.layers.Concatenate()([state_out, action_out])\n",
    "    output = tf.keras.layers.Dense(1024, activation=\"relu\")(output)\n",
    "    output = tf.keras.layers.BatchNormalization()(output)\n",
    "    output = tf.keras.layers.Dropout(0.5)(output)\n",
    "    output = tf.keras.layers.Dense(1, activation=\"linear\")(output)\n",
    "\n",
    "    # Create model\n",
    "    critic_model = tf.keras.Model([state_in, action_in], output)\n",
    "    \n",
    "    return critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1073b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actions(state_batch, gaus_mean, gaus_std):\n",
    "#     fixed_actions = fixed_model(state_batch, training=True)\n",
    "    residual_actions = residual_model(state_batch, training=True)\n",
    "#     actions = fixed_actions + residual_actions\n",
    "#     for action in actions:\n",
    "#         for i, value in enumerate(action):\n",
    "#             # Add Gaussian noise\n",
    "#             action += np.random.normal(gaus_mean, gaus_std)\n",
    "    return residual_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fbb19b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state, residual_action, gaus_mean, gaus_std):\n",
    "    # Get suggested action from actor\n",
    "    fixed_action = fixed_model.predict(np.array([state,]), verbose=0)[0]\n",
    "#     residual_action = residual_model.predict(np.array([state,]), verbose=0)[0]\n",
    "    action = fixed_action + residual_action\n",
    "    for i, value in enumerate(action):\n",
    "        # Add Gaussian noise\n",
    "        action += np.random.normal(gaus_mean, gaus_std)\n",
    "        # Clip action\n",
    "        if i == 0:\n",
    "            action[0] = np.clip(value, -1, 1)\n",
    "        else:\n",
    "            action[i] = np.clip(value, 0, 1)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c2740eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity, batch_size):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.replay_buffer = deque(maxlen=buffer_capacity)\n",
    "#         self.state_buffer = np.zeros((self.buffer_capacity, 84, 84, 1))\n",
    "#         self.action_buffer = np.zeros((self.buffer_capacity, 3))\n",
    "#         self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "#         self.next_state_buffer = np.zeros((self.buffer_capacity, 84, 84, 1))\n",
    "        \n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "#         index = self.buffer_counter % self.buffer_capacity\n",
    "        \n",
    "        self.replay_buffer.append(obs_tuple)\n",
    "#         self.state_buffer[index] = obs_tuple[0]\n",
    "#         self.action_buffer[index] = obs_tuple[1]\n",
    "#         self.reward_buffer[index] = obs_tuple[2]\n",
    "#         self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "#         self.buffer_counter += 1\n",
    "    \n",
    "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "#     @tf.function\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch\n",
    "    ):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = get_actions(next_state_batch, gaus_mean, gaus_std)\n",
    "            y = reward_batch + γ * critic_model(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = get_actions(state_batch, gaus_mean, gaus_std)\n",
    "            critic_value = critic_model([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, residual_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, residual_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        \n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "    \n",
    "        state_batch = tf.convert_to_tensor(np.array([step[0] for step in batch]).reshape((self.batch_size, 84, 84, 1)))\n",
    "        action_batch = tf.convert_to_tensor(np.array([step[1] for step in batch]).reshape((self.batch_size, 3)))\n",
    "        reward_batch = tf.convert_to_tensor(np.array([step[2] for step in batch]).reshape((self.batch_size, 1)))\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(np.array([step[3] for step in batch]).reshape((self.batch_size, 84, 84, 1)))\n",
    "        \n",
    "#         # Randomly sample indices\n",
    "#         batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "#         # Convert to tensors\n",
    "#         state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "#         action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "#         reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "#         reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "#         next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87fbeffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 09:29:40.531065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-05 09:29:40.551518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-05 09:29:40.551653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-05 09:29:40.552318: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-05 09:29:40.553116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-05 09:29:40.553213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-05 09:29:40.553289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-05 09:29:40.932255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-05 09:29:40.932397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-05 09:29:40.932482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-05 09:29:40.932548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-05-05 09:29:42.739857: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401\n",
      "2023-05-05 09:29:43.540524: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "episode 0 rewards: 905.1678714859338\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 1 rewards: 812.8014184397051\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 2 rewards: 829.9999999999847\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 3 rewards: 756.3513513513361\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 4 rewards: 619.2857142857044\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 5 rewards: 79.75728155340275\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 6 rewards: 787.7586206896402\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 7 rewards: 834.3286219081137\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 8 rewards: 841.0269360269207\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 9 rewards: 49.87632508834293\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 10 rewards: 897.8739926739821\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 11 rewards: 607.4539877300522\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 12 rewards: 852.1830985915326\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 13 rewards: 818.7931034482608\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 14 rewards: 698.2098765431969\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 15 rewards: 832.7566539923781\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 16 rewards: 882.099236641211\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 17 rewards: 865.1449275362226\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 18 rewards: 726.1920529801232\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 19 rewards: 823.727915194332\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 20 rewards: 747.4657534246434\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 21 rewards: 686.3504823150993\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 22 rewards: 761.2091503267809\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 23 rewards: 684.999999999989\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 24 rewards: 859.5454545454405\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 25 rewards: 69.91228070175896\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 26 rewards: 898.8373134328206\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 27 rewards: 886.9494584837469\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 28 rewards: 763.1081081080926\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 29 rewards: 787.1548821548668\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 30 rewards: 828.3576642335607\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 31 rewards: 728.9202657807208\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 32 rewards: 710.1948051947912\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 33 rewards: 826.4285714285566\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 34 rewards: 703.6348122866734\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 35 rewards: 900.3371647509499\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 36 rewards: 899.8812030075023\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 37 rewards: 861.5217391304255\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 38 rewards: 722.5675675675524\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 39 rewards: 851.5648854961727\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 40 rewards: 897.9664122137301\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 41 rewards: 834.629629629615\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 42 rewards: 805.3436426116683\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 43 rewards: 824.8606271776872\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 44 rewards: 900.6812030075025\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 45 rewards: 874.4656488549513\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "episode 46 rewards: 903.0012448132684\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 47 rewards: 708.2258064516043\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 48 rewards: 764.3155893535979\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 49 rewards: 714.2105263157761\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 50 rewards: 671.4473684210395\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 51 rewards: 676.2418300653449\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 52 rewards: 819.3835616438199\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 53 rewards: 857.8985507246281\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 54 rewards: 771.2207357859373\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 55 rewards: 899.7470588235126\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 56 rewards: 637.7327327327209\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 57 rewards: 899.4272727272579\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 58 rewards: 648.283582089541\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 59 rewards: 801.1937716262885\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 60 rewards: 643.1703470031418\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 61 rewards: 890.2398523985142\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 62 rewards: 628.0769230769134\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 63 rewards: 727.5255972696082\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 64 rewards: 422.06036745405567\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 65 rewards: 710.642633228825\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 66 rewards: 701.721311475398\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 67 rewards: 815.7142857142707\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 68 rewards: 681.3975155279369\n",
      "step 100\n",
      "step 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 69 rewards: 870.1567944250738\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 70 rewards: 648.0340557275455\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 71 rewards: 711.5573770491684\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 72 rewards: 748.003412969268\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 73 rewards: 887.2064056939381\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 74 rewards: 769.2384105960176\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 75 rewards: 727.3938223938095\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 76 rewards: 650.283018867909\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 77 rewards: 681.3975155279369\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 78 rewards: 628.5294117646969\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 79 rewards: 899.3528301886693\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 80 rewards: 709.7138047137902\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 81 rewards: 675.9677419354749\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 82 rewards: 756.3513513513358\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 83 rewards: 691.5853658536454\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 84 rewards: 881.9230769230618\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 85 rewards: 698.65079365078\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 86 rewards: 714.9999999999889\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 87 rewards: 604.0595611285145\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 88 rewards: 882.941176470571\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 89 rewards: 581.4705882352808\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 90 rewards: 872.8571428571273\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "episode 91 rewards: 907.3254237288005\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 92 rewards: 88.40611353712265\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 93 rewards: 772.857142857128\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 94 rewards: 705.6430868167068\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 95 rewards: 811.9767441860372\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 96 rewards: 687.3343848580326\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 97 rewards: 768.9455782312835\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 98 rewards: 759.2372881355803\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 99 rewards: 718.559322033886\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 100 rewards: 96.83673469388236\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 101 rewards: 787.352941176454\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 102 rewards: 674.4704049844097\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 103 rewards: 464.7667638483895\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "episode 104 rewards: 904.5699186991807\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 105 rewards: 899.4371647509498\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 106 rewards: 706.9169329073375\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 107 rewards: 391.1878453038581\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 108 rewards: 870.6488549618218\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 109 rewards: 831.0563380281529\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 110 rewards: 741.734693877542\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 111 rewards: 654.9999999999868\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 112 rewards: 717.4999999999868\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 113 rewards: 710.9210526315659\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 114 rewards: 897.6480620154956\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 115 rewards: 628.5294117646969\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 116 rewards: 694.8305084745634\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 117 rewards: 900.7093632958698\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 118 rewards: 844.2857142856988\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 119 rewards: 655.7692307692216\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 120 rewards: 629.9999999999884\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 121 rewards: 725.069204152242\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 122 rewards: 677.435897435886\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 123 rewards: 793.5017421602658\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 124 rewards: 671.7731629392864\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 125 rewards: 645.6249999999883\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 126 rewards: 700.4545454545315\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 127 rewards: 710.9210526315657\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 128 rewards: 889.3749999999867\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 129 rewards: 730.2427184465885\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 130 rewards: 893.5931558935179\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 131 rewards: 682.4390243902311\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 132 rewards: 848.609022556374\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 133 rewards: 557.6610644257623\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 134 rewards: 861.3636363636214\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 135 rewards: 858.5714285714133\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 136 rewards: 803.245614035072\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 137 rewards: 721.9934640522719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 138 rewards: 785.3986710963362\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 139 rewards: 726.874999999988\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 140 rewards: 751.4163822525434\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 141 rewards: 801.9072164948301\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 142 rewards: 768.9455782312839\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 143 rewards: 839.4827586206744\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 144 rewards: 826.7081850533691\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 145 rewards: 660.5555555555428\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 146 rewards: 817.0879120879014\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 147 rewards: 800.270270270254\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 148 rewards: 860.7195571955622\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 149 rewards: 734.9319727891062\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 150 rewards: 69.33566433566887\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 151 rewards: 771.1971830985763\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 152 rewards: 816.3475177304852\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 153 rewards: 807.7777777777615\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 154 rewards: 721.326530612236\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 155 rewards: 727.2996515679312\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 156 rewards: 42.54646840149032\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 157 rewards: 667.1951219512076\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 158 rewards: 833.3276450511776\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 159 rewards: 745.1360544217594\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 160 rewards: 879.7292418772483\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 161 rewards: 897.6093632958693\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 162 rewards: 841.1702127659463\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 163 rewards: 704.3630573248247\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 164 rewards: 810.9233449477221\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 165 rewards: 559.9707602339091\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 166 rewards: 770.9793814432838\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 167 rewards: 822.9104477611786\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 168 rewards: 603.8636363636251\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 169 rewards: 74.29133858268187\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 170 rewards: 796.0891089108819\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 171 rewards: 727.3684210526178\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 172 rewards: 779.1721854304544\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 173 rewards: 823.1494661921588\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 174 rewards: 776.7105263157758\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 175 rewards: 893.5496183206003\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 176 rewards: 701.7741935483782\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 177 rewards: 711.0200668896181\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 178 rewards: 609.477611940288\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 179 rewards: 802.8102189780867\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 180 rewards: 698.2098765431967\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 181 rewards: 863.0419580419436\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 182 rewards: 683.5016286644807\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 183 rewards: 815.852713178286\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 184 rewards: 790.7142857142708\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 185 rewards: 739.4827586206749\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 186 rewards: 803.2456140350721\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 187 rewards: 49.20062695925132\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 188 rewards: 50.5938697318046\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 189 rewards: 770.0519031141777\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 190 rewards: 755.9933774834346\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 191 rewards: 683.1350482314989\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 192 rewards: 73.42105263158348\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 193 rewards: 846.8181818181669\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 194 rewards: 838.0985915492795\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 195 rewards: 791.8613138685984\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 196 rewards: 776.5277777777617\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 197 rewards: 841.1702127659462\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 198 rewards: 753.5915492957598\n",
      "step 100\n",
      "step 200\n",
      "step 300\n",
      "step 400\n",
      "step 500\n",
      "step 600\n",
      "step 700\n",
      "step 800\n",
      "step 900\n",
      "episode 199 rewards: 709.4871794871684\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     reward_exp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.128\u001b[39m\u001b[38;5;241m*\u001b[39m(reward_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m20.65\u001b[39m\u001b[38;5;241m*\u001b[39mreward_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m919.0\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (buffer\u001b[38;5;241m.\u001b[39mbuffer_counter \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m):\n\u001b[0;32m---> 68\u001b[0m     \u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Check for termination\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (truncated) \u001b[38;5;129;01mor\u001b[39;00m (terminated) \u001b[38;5;129;01mor\u001b[39;00m (steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1500\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mround\u001b[39m(reward_sum) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mround\u001b[39m(reward_exp)):\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mBuffer.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         next_state_batch \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(np\u001b[38;5;241m.\u001b[39marray([step[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m batch])\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m#         # Randomly sample indices\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m#         batch_indices = np.random.choice(record_range, self.batch_size)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m#         reward_batch = tf.cast(reward_batch, dtype=tf.float32)\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m#         next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mBuffer.update\u001b[0;34m(self, state_batch, action_batch, reward_batch, next_state_batch)\u001b[0m\n\u001b[1;32m     43\u001b[0m     target_actions \u001b[38;5;241m=\u001b[39m get_actions(next_state_batch, gaus_mean, gaus_std)\n\u001b[1;32m     44\u001b[0m     y \u001b[38;5;241m=\u001b[39m reward_batch \u001b[38;5;241m+\u001b[39m γ \u001b[38;5;241m*\u001b[39m critic_model(\n\u001b[1;32m     45\u001b[0m         [next_state_batch, target_actions], training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     )\n\u001b[0;32m---> 47\u001b[0m     critic_value \u001b[38;5;241m=\u001b[39m \u001b[43mcritic_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_batch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     critic_loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39msquare(y \u001b[38;5;241m-\u001b[39m critic_value))\n\u001b[1;32m     50\u001b[0m critic_grad \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(critic_loss, critic_model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py:490\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39mcopied_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    488\u001b[0m   layout_map_lib\u001b[38;5;241m.\u001b[39m_map_subclass_model_variable(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout_map)\n\u001b[0;32m--> 490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/engine/base_layer.py:1014\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1014\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1017\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     94\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/engine/functional.py:458\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;129m@doc_controls\u001b[39m\u001b[38;5;241m.\u001b[39mdo_not_doc_inheritable\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    441\u001b[0m   \u001b[38;5;124;03m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m  In this case `call` just reapplies\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;124;03m      a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_internal_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/engine/functional.py:596\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    593\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[0;32m--> 596\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_id, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node\u001b[38;5;241m.\u001b[39mflat_output_ids, tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/engine/base_layer.py:1014\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1014\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1017\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     94\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/layers/normalization/batch_normalization.py:893\u001b[0m, in \u001b[0;36mBatchNormalizationBase.call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m   scale \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(scale, inputs\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 893\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_normalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_broadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m_broadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_dtype \u001b[38;5;129;01min\u001b[39;00m (tf\u001b[38;5;241m.\u001b[39mfloat16, tf\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[1;32m    897\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(outputs, inputs_dtype)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1076\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iterable_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m replace_iterable_params(args, kwargs, iterable_params)\n\u001b[0;32m-> 1076\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mapi_dispatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1078\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fixed_model = get_actor()\n",
    "residual_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "# Making the weights equal initially\n",
    "fixed_model.load_weights(\"./Downloads/my_checkpoint\")\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.0006\n",
    "actor_lr = 0.0005\n",
    "\n",
    "# Gauss parameters\n",
    "gaus_mean = 0\n",
    "gaus_std = 0.005\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "num_episodes = 500\n",
    "# Discount factor for future rewards\n",
    "γ = 0.975\n",
    "\n",
    "buffer = Buffer(20000, 32)\n",
    "\n",
    "env = gym.make(\"CarRacing-v2\", domain_randomize=False, autoreset=False)\n",
    "episode_rewards = np.empty(num_episodes)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation, info = env.reset()\n",
    "    # Stay still during initial zoom in\n",
    "    for _ in range(50):\n",
    "        action = [0.0, 0.0, 0.0]\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    steps = 0\n",
    "    reward_sum = 0\n",
    "    reward_size = 0\n",
    "    reward_exp = 936\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        steps += 1\n",
    "        if (steps % 100 == 0):\n",
    "            print(f\"step {steps}\")\n",
    "        # Get and preprocess state image.\n",
    "        prev_state_img = np.copy(observation)\n",
    "        prev_state_img = crop(prev_state_img)\n",
    "        prev_state_img = rgb_to_grey(prev_state_img).reshape((84,84,1))\n",
    "        \n",
    "        residual_action = residual_model.predict(np.array([prev_state_img,]), verbose=0)[0]\n",
    "        action = get_action(prev_state_img, residual_action, gaus_mean, gaus_std)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        state_img = np.copy(observation)\n",
    "        state_img = crop(state_img)\n",
    "        state_img = rgb_to_grey(state_img).reshape((84,84,1))\n",
    "        \n",
    "        buffer.record((prev_state_img, residual_action, reward, state_img))\n",
    "        \n",
    "         # Count steps and measure reward because terminated does not work\n",
    "        episode_reward += reward\n",
    "        if reward > 0:\n",
    "            reward_size = reward\n",
    "            reward_sum += reward\n",
    "            reward_exp = -2.128*(reward_size**2) + 20.65*reward_size + 919.0\n",
    "        \n",
    "        if (buffer.buffer_counter % 5 == 0) and (steps >= 32):\n",
    "            buffer.learn()\n",
    "        \n",
    "        # Check for termination\n",
    "        if (truncated) or (terminated) or (steps >= 1500) or (round(reward_sum) == round(reward_exp)):\n",
    "            episode_rewards[episode] = episode_reward\n",
    "            print(f\"episode {episode} rewards: {episode_reward}\")\n",
    "            residual_model.save_weights('./weights/my_actor4')\n",
    "            critic_model.save_weights('./weights/my_critic4')\n",
    "            break\n",
    "\n",
    "        prev_state_img = state_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80905951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 15:37:03.452149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-05 15:37:03.474071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-05 15:37:03.474206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-05 15:37:03.474527: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-05 15:37:03.475133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-05 15:37:03.475222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-05 15:37:03.475295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-05 15:37:03.848821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-05 15:37:03.848957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-05 15:37:03.849041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-05 15:37:03.849110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fcdcc758ee0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_model = get_actor()\n",
    "residual_model = get_actor()\n",
    "\n",
    "fixed_model.load_weights(\"./Downloads/my_checkpoint\")\n",
    "residual_model.load_weights(\"./weights/my_actor3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4174e831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 15:37:06.671674: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401\n",
      "2023-05-05 15:37:07.507238: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 reward: 71.66666666667138\n",
      "episode 1 reward: 645.9638554216764\n",
      "episode 2 reward: 890.5595667869959\n",
      "episode 3 reward: 721.3934426229391\n",
      "episode 4 reward: 798.9929328621771\n",
      "episode 5 reward: 851.8085106382867\n",
      "episode 6 reward: 861.8345323740848\n",
      "episode 7 reward: 871.1654135338176\n",
      "episode 8 reward: 886.3432835820739\n",
      "episode 9 reward: 800.9107806691321\n",
      "episode 10 reward: 855.5300353356746\n",
      "episode 11 reward: 720.8844765342858\n",
      "episode 12 reward: 750.878136200704\n",
      "episode 13 reward: 897.7272727272574\n",
      "episode 14 reward: 47.85714285714665\n",
      "episode 15 reward: 897.2480620154956\n",
      "episode 16 reward: 682.4193548387005\n",
      "episode 17 reward: 828.0769230769126\n",
      "episode 18 reward: 792.7551020408074\n",
      "episode 19 reward: 701.7213114753979\n",
      "episode 20 reward: 899.4812030075024\n",
      "episode 21 reward: 848.0604982206286\n",
      "episode 22 reward: 687.0069204152154\n",
      "episode 23 reward: 685.1857585139228\n",
      "episode 24 reward: 866.403508771914\n",
      "episode 25 reward: 685.6451612903136\n",
      "episode 26 reward: 712.9470198675401\n",
      "episode 27 reward: 871.1654135338174\n",
      "episode 28 reward: 724.4444444444297\n",
      "episode 29 reward: 849.6366782006835\n",
      "episode 30 reward: 850.0549450549344\n",
      "episode 31 reward: 674.230769230755\n",
      "episode 32 reward: 774.2579505300218\n",
      "episode 33 reward: 797.8571428571278\n",
      "episode 34 reward: 632.2727272727119\n",
      "episode 35 reward: 727.3684210526181\n",
      "episode 36 reward: 79.90494296578427\n",
      "episode 37 reward: 650.5830388692457\n",
      "episode 38 reward: 671.871165644162\n",
      "episode 39 reward: 767.0689655172265\n",
      "episode 40 reward: 833.0575539568187\n",
      "episode 41 reward: 661.0975609755976\n",
      "episode 42 reward: 599.8051948051854\n",
      "episode 43 reward: 741.0655737704807\n",
      "episode 44 reward: 809.059040590396\n",
      "episode 45 reward: 693.1944444444326\n",
      "episode 46 reward: 841.1702127659463\n",
      "episode 47 reward: 871.2921348314496\n",
      "episode 48 reward: 867.9629629629483\n",
      "episode 49 reward: 868.7681159420197\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "env = gym.make(\"CarRacing-v2\", domain_randomize=False, autoreset=False)\n",
    "episode_rewards = np.zeros(50)\n",
    "for i in range(50):\n",
    "    observation, info = env.reset()\n",
    "    for _ in range(50):\n",
    "        action = [0.0, 0.0, 0.0]\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "    steps = 0\n",
    "    reward_sum = 0\n",
    "    num_rewards = 0\n",
    "    reward_size = 0\n",
    "    reward_exp = 0\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        state_img = crop(observation)\n",
    "        state_img = rgb_to_grey(state_img).reshape((84,84,1))\n",
    "        action = fixed_model.predict(np.array([state_img,]), verbose=0)[0] + residual_model.predict(np.array([state_img,]), verbose=0)[0]\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        steps += 1\n",
    "        total_reward += reward\n",
    "        if reward > 0:\n",
    "            reward_size = reward\n",
    "            reward_sum += reward\n",
    "            reward_exp = -2.128*(reward_size**2) + 20.65*reward_size + 919.0\n",
    "        if terminated or truncated or (steps >= 1500) or (round(reward_sum) == round(-2.128*(reward_size**2) + 20.65*reward_size + 919.0)):\n",
    "            episode_rewards[i] = total_reward\n",
    "            print(f\"episode {i} reward: {total_reward}\")\n",
    "#             print(round(-2.128*(reward_size**2) + 20.65*reward_size + 919.0,2))\n",
    "#             print(round(reward_sum,2))\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ecb7a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 71.66666667, 645.96385542, 890.55956679, 721.39344262,\n",
       "       798.99293286, 851.80851064, 861.83453237, 871.16541353,\n",
       "       886.34328358, 800.91078067, 855.53003534, 720.88447653,\n",
       "       750.8781362 , 897.72727273,  47.85714286, 897.24806202,\n",
       "       682.41935484, 828.07692308, 792.75510204, 701.72131148,\n",
       "       899.48120301, 848.06049822, 687.00692042, 685.18575851,\n",
       "       866.40350877, 685.64516129, 712.94701987, 871.16541353,\n",
       "       724.44444444, 849.6366782 , 850.05494505, 674.23076923,\n",
       "       774.25795053, 797.85714286, 632.27272727, 727.36842105,\n",
       "        79.90494297, 650.58303887, 671.87116564, 767.06896552,\n",
       "       833.05755396, 661.09756098, 599.80519481, 741.06557377,\n",
       "       809.05904059, 693.19444444, 841.17021277, 871.29213483,\n",
       "       867.96296296, 868.76811594])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c0b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_model = get_actor()\n",
    "residual_model = get_actor()\n",
    "\n",
    "fixed_model.load_weights(\"./checkpoints/my_checkpoint\")\n",
    "residual_model.load_weights(\"./weights/my_actor3\")\n",
    "\n",
    "env = gym.make(\"CarRacing-v2\", domain_randomize=False, autoreset=False, render_mode=\"human\")\n",
    "episode_rewards = np.zeros(50)\n",
    "for i in range(50):\n",
    "    observation, info = env.reset()\n",
    "    for _ in range(50):\n",
    "        action = [0.0, 0.0, 0.0]\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "    steps = 0\n",
    "    reward_sum = 0\n",
    "    num_rewards = 0\n",
    "    reward_size = 0\n",
    "    reward_exp = 0\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        state_img = crop(observation)\n",
    "        state_img = rgb_to_grey(state_img).reshape((84,84,1))\n",
    "        action = fixed_model.predict(np.array([state_img,]), verbose=0)[0] + residual_model.predict(np.array([state_img,]), verbose=0)[0]\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        steps += 1\n",
    "        total_reward += reward\n",
    "        if reward > 0:\n",
    "            reward_size = reward\n",
    "            reward_sum += reward\n",
    "            reward_exp = -2.128*(reward_size**2) + 20.65*reward_size + 919.0\n",
    "        if terminated or truncated or (steps >= 1500) or (round(reward_sum) == round(-2.128*(reward_size**2) + 20.65*reward_size + 919.0)):\n",
    "            episode_rewards[i] = total_reward\n",
    "            print(f\"episode {i} reward: {total_reward}\")\n",
    "#             print(round(-2.128*(reward_size**2) + 20.65*reward_size + 919.0,2))\n",
    "#             print(round(reward_sum,2))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8214b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_model = get_actor()\n",
    "residual_model = get_actor()\n",
    "\n",
    "fixed_model.load_weights(\"./checkpoints/my_checkpoint\")\n",
    "residual_model.load_weights(\"./weights/my_actor2\")\n",
    "\n",
    "env = gym.make(\"CarRacing-v2\", domain_randomize=False, autoreset=False, render_mode=\"human\")\n",
    "episode_rewards = np.zeros(50)\n",
    "for i in range(50):\n",
    "    observation, info = env.reset()\n",
    "    for _ in range(50):\n",
    "        action = [0.0, 0.0, 0.0]\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "    steps = 0\n",
    "    reward_sum = 0\n",
    "    num_rewards = 0\n",
    "    reward_size = 0\n",
    "    reward_exp = 0\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        state_img = crop(observation)\n",
    "        state_img = rgb_to_grey(state_img).reshape((84,84,1))\n",
    "        action = fixed_model.predict(np.array([state_img,]), verbose=0)[0] + residual_model.predict(np.array([state_img,]), verbose=0)[0]\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        steps += 1\n",
    "        total_reward += reward\n",
    "        if reward > 0:\n",
    "            reward_size = reward\n",
    "            reward_sum += reward\n",
    "            reward_exp = -2.128*(reward_size**2) + 20.65*reward_size + 919.0\n",
    "        if terminated or truncated or (steps >= 1500) or (round(reward_sum) == round(-2.128*(reward_size**2) + 20.65*reward_size + 919.0)):\n",
    "            episode_rewards[i] = total_reward\n",
    "            print(f\"episode {i} reward: {total_reward}\")\n",
    "#             print(round(-2.128*(reward_size**2) + 20.65*reward_size + 919.0,2))\n",
    "#             print(round(reward_sum,2))\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
